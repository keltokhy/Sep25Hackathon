[base]
package = metta
env_name = metta 
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 64
num_workers = 16

[env]
render_mode = auto
ore_reward = 0.17088483842567775
battery_reward = 0.9882859711234822
heart_reward = 1.0

[train]
total_timesteps = 300_000_000
batch_size = auto
adam_beta1 = 0.8923106632311335
adam_beta2 = 0.9632470625784862
adam_eps = 1.3537431449843922e-7
clip_coef = 0.14919147162017737
ent_coef = 0.016700174334611493
gae_lambda = 0.8443676864928215
gamma = 0.997950174315581
learning_rate = 0.018470110879570414
max_grad_norm = 2.572849891206465
minibatch_size = 32768
bptt_horizon = 64
prio_alpha = 0.7918451491719373
prio_beta0 = 0.5852686803034238
vf_clip_coef = 0.1569624916309049
vf_coef = 3.2211333828684454
vtrace_c_clip = 2.134490283650365
vtrace_rho_clip = 2.296343917695581

[sweep]
metric = agent/heart.gained

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e8
max = 5e8
mean = 3e8
scale = auto

[sweep.env.ore_reward]
distribution = uniform
min = 0.0
mean = 0.25
max = 1.0
scale = auto

[sweep.env.battery_reward]
distribution = uniform
min = 0.0
mean = 0.5
max = 1.0
scale = auto

[sweep.env.heart_reward]
distribution = uniform
min = 0.0
mean = 1.0
max = 1.0
scale = auto
