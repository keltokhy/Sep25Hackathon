[base]
package = ocean
env_name = puffer_drone_pp
policy_name = Policy
rnn_name = Recurrent

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[vec]
num_envs = 24

[env]
num_envs = 24 # 16
num_drones = 64 # 64
max_rings = 10

reward_min_dist = 1.3159451723909112
reward_max_dist = 83.14960592300233
dist_decay = 0.5

w_position = 1.2303854103933083
w_velocity = 0.12632002850721588
w_stability = 1.8328041440802467
w_approach = 2.4493223157596984
w_hover = 1.6429730342663187

pos_const = 0.6233603728023545
pos_penalty = 0.03827543428980447

grip_k_min = 1.0
grip_k_max = 17.887758597919266
grip_k_decay = 0.09049941256843744

box_base_density = 50.0
box_k_growth = 0.02

reward_hover = 0.25
reward_grip = 0.5
reward_deliv = 0.75

[train]
adam_beta1 = 0.8999428142702635 # 0.9610890980775877
adam_beta2 = 0.9895934228748904 # 0.9999260775286266
adam_eps = 1.214178387931685e-06 # 7.782906079040132e-10
anneal_lr = true
batch_size = auto
bptt_horizon = 64
checkpoint_interval = 200
clip_coef = 0.6176880492374869 # 0.05982655642208556
ent_coef = 0.07123566778619919 # 0.002465076521024325
gae_lambda = 0.9892341546158178 # 0.9641173414828333
gamma = 0.987853479502188 # 0.997472126425902
learning_rate = 0.006047018496816363 # 0.010933756713881205
#learning_rate = 0.005
max_grad_norm = 3.049550252257216 # 1.6317688647793107
max_minibatch_size = 32768
minibatch_size = 16384
prio_alpha = 0.8419064782199285 # 0.8968873016577552
prio_beta0 = 0.9572979813172062 # 0.8672928227817938
total_timesteps = 200_000_000
update_epochs = 1
#use_rnn = false
vf_clip_coef = 1.242364910018791 # 0.5869845581530236
vf_coef = 5.0 # 2.1319065538539963
vtrace_c_clip = 0.918044333543304 # 2.714930379733876
vtrace_rho_clip = 3.000888799918418 # 3.8183814893708057

[sweep]
method = Protein
metric = perfect_deliv
goal = maximize
downsample = 0

[sweep.env.w_position]
distribution = uniform
min = 0.0
max = 1.5
mean = 1.23
scale = auto

[sweep.env.w_velocity]
distribution = uniform
min = 0.0
max = 1.5
mean = 0.13
scale = auto

[sweep.env.w_stability]
distribution = uniform
min = 0.0
max = 2.5
mean = 1.83
scale = auto

[sweep.env.w_approach]
distribution = uniform
min = 0.0
max = 2.5
mean = 2.4
scale = auto

[sweep.env.w_hover]
distribution = uniform
min = 0.0
max = 2.0
mean = 1.64
scale = auto

[sweep.env.reward_min_dist]
distribution = uniform
min = 0.1
max = 5.0
mean = 1.6
scale = auto

[sweep.env.reward_max_dist]
distribution = uniform
min = 60.0
max = 100.0
mean = 77.0
scale = auto

[sweep.env.dist_decay]
distribution = uniform
min = 0.01
max = 0.5
mean = 0.5
scale = auto

[sweep.env.pos_const]
distribution = uniform
min = 0.001
max = 1.0
mean = 0.63
scale = auto

[sweep.env.pos_penalty]
distribution = uniform
min = 0.001
max = 0.25
mean = 0.04
scale = auto

[sweep.env.grip_k_max]
distribution = uniform
min = 1.0
max = 20.0
mean = 15.0
scale = auto

[sweep.env.grip_k_decay]
distribution = uniform
min = 0.01
max = 0.15
mean = 0.095
scale = auto

[sweep.env.box_base_density]
distribution = uniform
min = 25.0
max = 100.0
mean = 50.0
scale = auto

[sweep.env.box_k_growth]
distribution = uniform
min = 0.005
max = 0.5
mean = 0.02
scale = auto

[sweep.env.reward_hover]
distribution = uniform
min = 0.0
max = 0.5
mean = 0.25
scale = auto

[sweep.env.reward_grip]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto

[sweep.env.reward_deliv]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.75
scale = auto
