[base]
package = ocean
env_name = puffer_drone_pickplace
policy_name = Policy
rnn_name = Recurrent


[vec]
backend = Multiprocessing
num_workers = 1
num_envs = 1

[env]
num_envs = 1024
num_drones = 1
num_objects = 1
num_targets = 1
world_size = 2.0
max_height = 1.5
max_steps = 500
reward_approach = 0.01
reward_complete = 1.0
reward_grasp = 1.0
reward_place = 1.0
penalty_no_progress = -0.1
penalty_time = -0.001

; [train]
; adam_beta1 = 0.9
; adam_beta2 = 0.999
; adam_eps = 1e-7
; anneal_lr = true
; bptt_horizon = 16
; checkpoint_interval = 200
; clip_coef = 0.2
; ent_coef = 0.001
; gae_lambda = 0.95
; gamma = 0.99
; learning_rate = 0.0003
; max_grad_norm = 1.6317688647793107
; batch_size = 131072
; max_minibatch_size = 32768
; minibatch_size = 32768
; prio_alpha = 0.8968873016577552
; prio_beta0 = 0.8672928227817938
; total_timesteps = 100_000_000
; update_epochs = 1
; vf_clip_coef = 0.5869845581530236
; vf_coef = 2.1319065538539963
; vtrace_c_clip = 2.714930379733876
; vtrace_rho_clip = 3.8183814893708057

[sweep.env.reward_approach]
distribution = uniform
min = 0.001
max = 0.05
mean = 0.01
scale = auto

[sweep.env.reward_complete]
distribution = uniform
min = 0.25
max = 1.0
mean = 1.0
scale = auto

[sweep.env.reward_grasp]
distribution = uniform
min = 0.25
max = 1.0
mean = 1.0
scale = auto

[sweep.env.reward_place]
distribution = uniform
min = 0.25
max = 1.0
mean = 1.0
scale = auto

[sweep.env.penalty_no_progress]
distribution = uniform
min = -0.25
max = -0.01
mean = -0.1
scale = auto

[sweep.env.penalty_time]
distribution = uniform
min = -0.1
max = -0.0001
mean = -0.001
scale = auto

[train]
total_timesteps = 100_000_000
learning_rate = 0.0003
minibatch_size = 8192
rollout_length = 128
update_epochs = 1
gamma = 0.99
gae_lambda = 0.95
clip_range = 0.2
value_loss_coef = 0.5
entropy_coef = 0.01
max_grad_norm = 0.5
