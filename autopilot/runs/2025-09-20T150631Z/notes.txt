Run: 2025-09-20T150631Z (full baseline)
Summary:
- Steps: 11.0M (epoch 349), SPS: 129.6K, uptime ~2m56s
- success_rate: 0.000, mean_reward: 15.12, collision_rate: 0.00117
- Episode length: 299.15, score: 15.12, oob ~0.71 (from UI panel)
- Approx KL ~0.000, clipfrac ~0.000, entropy ~24.46
- Device: mps; vec 28/56, env 4×8; update_epochs: 8; optimizer: muon; anneal_lr: true
- Derived batch: (4×8×56)×16 = 28,672; minibatch = max_minibatch = 28,672
- Divisibility: 56 % 28 == 0 (OK)
- Resource panel: CPU ~357% (under host capacity), GPU ~0% (expected for mps)

Trend check (recent runs):
- success_rate remained 0.0 across 10–30M timesteps and ent_coef ∈ {0.02, 0.04, 0.06, 0.08, 0.10, 0.12}.
- mean_reward varied 8.44–15.12; collision_rate stayed low (≈0.0005–0.0021).
- Longer runs (20–30M) did not improve success_rate; SPS 120–142K stable.

Theory:
- KL≈0 and clipfrac≈0 suggest under-updating (updates too small to move the policy); high entropy (≈24.5) and LR annealing likely keep the policy diffuse near the end of training.
- Value loss is tiny; vf_coef=2.0 may be fine, but with small effective policy steps the agent fails to discover/retain successful behaviors (consistent zero successes and high OOB).

Action (next run overrides):
- Reduce exploration pressure and increase per-update learning: ent_coef 0.02; update_epochs 16; keep LR 3e-3 with anneal_lr=true; double total_timesteps to 20M; continue from best.
- Rationale: decreases entropy to focus policy, and more epochs per batch should raise approx_kl and clipfrac above ~0 without destabilizing (clip=0.2, max_grad_norm=1.5).

What to watch next:
- KL and clipfrac should rise above ~0; monitor success_rate and OOB.
- If KL remains ~0: consider switching optimizer to adamw and/or disabling LR anneal for a phase; alternatively raise bptt_horizon to 32 to propagate returns further.
