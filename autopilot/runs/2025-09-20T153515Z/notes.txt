Run 2025-09-20T153515Z — continue from best; baseline_full

Summary
- Config: mps, vec 28/56, env 4×8, bptt 16
- Derived batch: 4×8×56×16 = 28,672 (minibatch=max=28,672)
- Throughput: SPS ~124.6k; agent_steps 11,038,720; epoch 349
- Key metrics: success_rate 0.0; mean_reward 7.43; collision_rate 0.00073; episode_len ~149
- Trainer stats (tail): approx_kl ~0.000; clipfrac ~0.000; entropy ~32.73; value_loss ~0.006–0.007; explained_var ~0.85
- Utilisation panel: CPU ~355%; GPU 0.0%; DRAM ~33%; VRAM 0.0%

Context vs recent runs
- Recent full baselines show mean_reward typically 12–15 with success_rate ~0 (no confirmed successes yet) at 10–20M steps.
- This run underperformed on mean_reward (7.43) despite good SPS, suggesting weak policy updates rather than throughput bottlenecks.
- best.json points to a checkpoint reporting inflated success_rate (>1.0), likely a logging/aggregation glitch; warm-starting from “best” may not be reliable.

Diagnosis / Theory
- clipfrac ~0 and approx_kl ~0 indicate the policy ratio rarely hits the clip region and per-update shifts are tiny. With entropy high (~32), the policy stays diffuse, which aligns with low success and high OOB (~0.85 in the TUI).
- Throughput is healthy; instability isn’t the issue. The optimizer/hyperparams are probably too conservative per batch.

Action taken (next_config.json)
- Lower entropy pressure and increase per-batch learning:
  - train.ent_coef: 0.12 -> 0.10
  - train.update_epochs: 12 -> 16
  - train.total_timesteps: 10M -> 20M (still within runtime budget)
- Resume policy: continue from latest (not best) to avoid polluted “best” metric; save_strategy remains best.

Rationale
- Reducing ent_coef nudges the policy to commit more (less dithering). More update_epochs increases policy improvement per batch, which should raise approx_kl and clipfrac from near-zero, translating to larger actual updates without changing LR.
- Extending to 20M steps gives the agent more opportunity to translate updates into reward without drastically lengthening the run.

Checks
- vec.num_envs % vec.num_workers = 56 % 28 = 0 (OK)
- Derived batch constraint satisfied; no topology change in this iteration.
- Device remains mps for speed; deterministic toggles unchanged.

Next steps (if results still stall)
- If clipfrac remains near zero, consider: increase clip_coef to 0.18–0.20 or switch optimizer to adamw (same LR) to boost step magnitude.
- If OOB stays high, try increasing gamma to 0.995–0.997 to propagate longer-horizon shaping and/or lower vf_coef from 2.0 to 1.5 to emphasize policy learning.
