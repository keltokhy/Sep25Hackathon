2025-09-20T14:36Z — full run (warm start from latest)

Summary
- Steps: 20.93M; Epoch: 698; SPS: ~121.7k
- success_rate: 0.0000; mean_reward (score): ~15.00
- collision_rate: 0.00061; episode_length: ~43.51
- CPU panel: ~357% (UI aggregate); GPU panel: 0.0%; DRAM ~33%
- Entropy: ~18.7; approx_kl: ~0.000; clipfrac: ~0.000; explained_variance: ~0.963

Topology & batch
- device: mps; vec: workers 28 / envs 56; env: 4×8 drones
- bptt_horizon: 16 → derived batch = 4×8×56×16 = 28,672 (minibatch=max)
- Divisibility check: 56 % 28 == 0 ✅

What changed in this run
- Mode: full baseline; resume_mode=continue from latest (auto-injected load path)
- Optimizer: muon; lr: 3e-3; ent_coef: 0.10; update_epochs: 1 (baseline)

Observations
- success_rate remains 0 despite low collision_rate and stable losses.
- Very high entropy (~18.7) and clipfrac≈0 indicate overly diffuse policy and tiny effective updates; agent appears to wander (oob≈0.96), not progressing toward pickup/delivery (perfect_grip/deliv=0, rings_passed=0).
- Throughput is healthy for this topology (SPS ~120k). CPU utilization shown in the UI (~357%) likely reflects only the main process; worker utilization looks adequate given SPS.

Theory
- High exploration pressure (ent_coef=0.10) is keeping the policy too stochastic to achieve successes, even with long training. With update_epochs=1, each batch imparts low signal; combined with large clip range and near-zero KL, learning per update is muted.

Next step (proposal staged in proposals/next_config.json)
- Reduce entropy to encourage exploitation: train.ent_coef: 0.10 → 0.02.
- Modestly increase per-batch learning: train.update_epochs: 1 → 2.
- Hold learning_rate (0.003), clip_coef (0.3), topology, and seed constant to isolate effects.
- Warm start from strongest checkpoint: autopilot.resume_mode=continue; resume_from=best; save_strategy=best.

Expected outcome
- Lower entropy should reduce oob and increase directed behavior, raising mean_reward and enabling first non-zero success events without destabilizing training. Two epochs should improve sample efficiency while keeping RNN stability.

Follow-ups if no improvement
- Nudge lr to 0.0025 (stability) or 0.0035 (faster learning) depending on KL/clipfrac response.
- If entropy remains high, reduce ent_coef further to 0.01.
- If successes still zero, consider gamma=0.995 and/or slight increase in env.num_drones to stress policy capacity.
