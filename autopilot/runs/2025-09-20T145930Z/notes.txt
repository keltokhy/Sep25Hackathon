Run 2025-09-20T145930Z — full baseline

Summary
- Env: puffer_drone_pp; device `mps`
- Topology: `vec.num_workers` 28, `vec.num_envs` 56 (divisible ✔), `env.num_envs` 4, `env.num_drones` 8
- Derived batch: 4 × 8 × 56 × 16 = 28672; `batch_size = minibatch_size = max_minibatch_size` ✔
- PPO: lr 0.0025, ent_coef 0.06, update_epochs 4, clip_coef 0.12, bptt_horizon 16; optimizer muon; anneal_lr on; deterministic on
- Throughput/util: SPS ≈ 120.8k; CPU ≈ 357%; GPU (MPS) ≈ 0% (likely under-reported on macOS)

Key metrics
- success_rate 0.00; mean_reward 12.90; episode_length 52.18
- collision_rate 5.36e-4 (low); OOB ≈ 0.951 (very high)
- value head strong (explained_var ≈ 0.926); approx_kl ≈ 0.0; clipfrac ≈ 0.0; policy_loss small

Context vs recent runs
- 14:39Z (lr=3e-3, ent=0.02, upd=2, clip=0.12) mean_reward 13.12
- 14:43Z (lr=3e-3, ent=0.10, upd=1, clip=0.30) mean_reward 13.47 (best of set)
- 14:48Z (lr=3e-3, ent=0.12, upd=2) mean_reward 8.44 (entropy too high degraded)
- 14:55Z (lr=2e-3, ent=0.08, upd=2) mean_reward 12.40
- 14:59Z (this run config variant: lr=2.5e-3, ent=0.06, upd=4) mean_reward 12.90

Theory
- approx_kl ≈ 0 and clipfrac ≈ 0 indicate updates are overly conservative; combined with relatively high entropy the policy wanders and goes OOB (≈95%) without improving success.
- Lowering entropy while increasing optimization pressure (more epochs and a slightly higher lr and clip window) should drive more decisive updates and reduce OOB without destabilizing.

Next step (staged in proposals/next_config.json)
- train.learning_rate: 0.003 (from 0.0025)
- train.ent_coef: 0.04 (from 0.06)
- train.update_epochs: 8 (from 4)
- train.clip_coef: 0.20 (from 0.12)
- autopilot.resume_mode: continue; resume_from: best; save_strategy: best

Expectations
- Keep topology and batch (28672) unchanged for comparability; anticipate lower OOB and higher mean_reward. If clipfrac/kl remain near zero, consider raising update_epochs further or modestly increasing batch/agents next.
