add: lower lr to 1.8e-3; extend to 30M; continue from best

Summary
- Mode: full (baseline_full), resume=best, save=best
- Steps: 20.93M; SPS: 127.2K; Uptime: ~4m 14s
- Utilisation panel: CPU ~357%, GPU (mps) 0%, DRAM ~34%, VRAM 0%
- Topology: vec 28/56, env 4×8, bptt 16
- Divisibility: 56 % 28 == 0 (ok)
- Derived batch: 4×8×56×16 = 28672 (matches config/minibatch/max)
- Metrics: mean_reward 462.53, success_rate 1647.37, collision_rate 0.0050, episode_length 853.85

Recent trend (last 3 runs)
- 2025-09-20T161602Z (10M, lr=2.5e-3, ent=0.01): mean_reward 474.92, collision 0.0064
- 2025-09-20T162017Z (20M, lr=2.5e-3, ent=0.12): mean_reward 250.57, collision 0.00235
- 2025-09-20T162635Z (20M, lr=2.5e-3, ent=0.06): mean_reward 139.37, collision 0.00196
- 2025-09-20T163325Z (20M, lr=2.0e-3, ent=0.01): mean_reward 462.53, collision 0.00500

Theory
- Raising entropy (0.06–0.12) increased exploration and materially hurt exploitation quality (reward and success), despite slightly lower collision rates.
- Returning ent_coef to 0.01 restored policy quality. The slight LR drop from 2.5e-3 to 2.0e-3 likely reduced jitter without hurting convergence.

Next step (proposal in proposals/next_config.json)
- Keep topology fixed for comparability; warm-start from best.
- Lower learning_rate modestly to 1.8e-3 to stabilise late-stage updates.
- Extend total_timesteps to 30M to allow consolidation at low entropy.
- Expectation: higher mean_reward and stable/low collision_rate with similar SPS (~120–130K). If SPS degrades or reward plateaus, consider increasing update_epochs (to 6) in a follow-up.

Repro notes
- Device mps; torch_deterministic=true; anneal_lr=true; optimizer=muon.
- Env: puffer_drone_pp; checkpoints every 200 iters; batch/minibatch 28672.
