Run 2025-09-20T151134Z — full baseline; resume=best; ent=0.02; lr=3e-3; muon; 20M steps.

Summary
- Steps: 20.93M; Epoch: 698; SPS: ~125.3K (panel ranged 37–125K near end)
- Metrics: success_rate≈285.60, mean_reward≈438.33, collision_rate≈0.00326
- Episode length≈844.6; explained_variance≈0.796; approx_kl≈0.0; clipfrac≈0.0
- Utilisation panel: CPU≈340%, GPU(MPS)≈0%, DRAM≈33%, VRAM≈0%

Config & Topology
- vec.num_workers=28; vec.num_envs=56 (divisible ✓)
- env.num_envs=4; env.num_drones=8; bptt=16
- Derived batch: 4×8×56×16=28672; minibatch=max_minibatch=28672 (✓)
- Optimizer=muon; anneal_lr=true; deterministic=true; precision=float32

Context vs recent runs
- Prior runs with higher entropy (0.04–0.08) and similar LR yielded very low reward (mean≈12–15) and zero “success_rate”.
- This run lowered entropy to 0.02 and warm-started from best, driving a large jump in mean_reward (+400+) with a modest rise in collision_rate (~0.0033 vs ~0.001).

Theory
- Lowering entropy and resuming from a strong checkpoint improved task completion and reward. However, approx_kl≈0 and clipfrac≈0 suggest policy updates are extremely small at the end of training (potentially saturated or overly conservative updates with current clip_coef and epochs).
- To further improve quality (higher reward, fewer collisions), lean into exploitation (lower ent_coef) while slightly increasing update pressure (more epochs) and tightening clipping to control instability.

Next iteration (proposal written to proposals/next_config.json)
- Keep topology (28/56, 4×8) for reproducibility; continue from best.
- train.learning_rate → 0.0025 (slight decay for fine-tune stability)
- train.ent_coef → 0.01 (less exploration; focus on exploitation)
- train.update_epochs → 20 (more gradient passes per batch)
- train.clip_coef → 0.15 (tighter clipping to avoid large policy shifts)
- total_timesteps → 20,000,000; keep batch/minibatch/max_minibatch at 28,672 (derived)

Checks & notes
- vec.num_envs % vec.num_workers == 0 (56 % 28 == 0) ✓
- Batch sizing matches formula ✓
- If SPS drops in next run or CPU utilisation remains low, consider pushing concurrency (84 vec envs) in a later iteration; for this step, prioritised policy quality and stability.
