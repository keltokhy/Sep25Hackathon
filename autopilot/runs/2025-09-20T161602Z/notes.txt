Run: 2025-09-20T161602Z (full mode, baseline_full)
Warm start: continue from best (autopilot/runs/best.json)

Config highlights
- Device: mps; optimizer: muon; anneal_lr: true; update_epochs: 2; clip_coef: 0.12; max_grad_norm: 1.5
- Topology: vec 28/56, env 4×8; bptt 16
- Derived batch: 4 × 8 × 56 × 16 = 28,672 (minibatch = max_minibatch = 28,672)
- Divisibility: 56 % 28 == 0 ✓

Outcome (end of run)
- mean_reward: 474.92 (best recent: 496.56 @ 2025-09-20T160433Z)
- success_rate: 1255.57 (best recent: 1258.92)
- collision_rate: 0.00640 (best recent: 0.00502) — higher is worse
- episode_length: 875.72 (best recent: 882.86)
- SPS: 132.3k (prior full runs ~124–126k)
- Util panel (indicative): CPU ~357%, GPU(MPS) ~0%, DRAM ~34%, VRAM ~0%

Recent trend (last 3 full runs)
- 16:04:33Z (lr 2.5e-3, 10M): reward 496.56, coll 0.00502, succ 1258.92 (best)
- 16:10:21Z (lr 2.0e-3, 20M): reward 481.55, coll 0.00457, succ 1226.13
- 16:16:02Z (this, lr 2.5e-3, 10M, resumed): reward 474.92, coll 0.00640, succ 1255.57

Theory
- Slight regression in reward with higher collision rate suggests instability when resuming with only 2 PPO epochs and clip=0.12. The 2e7-timestep run reduced collisions but also lowered reward, hinting the policy needs more careful policy updates rather than just lower LR.

Plan for next iteration
- Keep lr at 2.5e-3 (performed best) but increase PPO update_epochs to 3 for better per-batch improvement.
- Tighten clip_coef to 0.10 and reduce max_grad_norm to 1.0 to stabilize updates and target lower collision rate without sacrificing reward.
- Extend total_timesteps to 20M to allow the above adjustments to consolidate.
- Continue from best checkpoint; keep save_strategy=best.

Proposed overrides (staged in proposals/next_config.json)
- train: learning_rate=0.0025, update_epochs=3, clip_coef=0.10, max_grad_norm=1.0, total_timesteps=20_000_000, batch/minibatch/max=28_672
- env/vec: unchanged (4×8, workers 28, vec envs 56)

Next checks
- Watch collision_rate and mean_reward over the first 10M steps; expect collision_rate to drop toward ~0.005 while reward approaches or exceeds ~495 if theory holds.
- If collisions remain elevated, consider lr 2.3e-3 or vf_coef→1.5 next; if reward stalls, revert clip to 0.12.
