Run summary (full, resume=best):

- Outcome: success_rate 0.000, mean_reward 13.115, collision_rate 0.00146.
- Throughput: SPS 125.6k; agent_steps 11.0M; epoch 349; uptime ~1m50s.
- Utilization panel: CPU ~357%, GPU 0%, DRAM ~33%, VRAM 0% (device=mps). 
- Config deltas vs baseline_full: ent_coef=0.02, update_epochs=2 (others default).
- Topology: vec 28/56; env 4×8; bptt 16.
- Derived batch: 4 × 8 × 56 × 16 = 28,672. Divisibility check: 56 % 28 == 2 ✓.

Context across recent runs:

- 2025-09-20T143330Z (best): ent_coef=0.10, update_epochs=1, clip_coef=0.3, total_timesteps=20M → mean_reward 15.00, success_rate 0.0, sps 121.7k.
- 2025-09-20T142823Z: ent_coef=0.08, update_epochs=4, clip_coef=0.12, 10M → mean_reward 14.61, success_rate 0.0, sps 125.1k.
- 2025-09-20T140553Z: ent_coef=0.12, update_epochs=1, 120M → mean_reward 14.32, success_rate 0.0, sps 126.3k.
- 2025-09-20T140103Z: ent_coef=0.12, update_epochs=1, 10M → mean_reward 2.61, success_rate 0.0, sps 106.4k.

Interpretation / theory:

- Dropping entropy (0.02) with 10M steps likely over-constrained exploration; reward dipped vs 0.10 and episode_length spiked, while success_rate stayed 0.0. Best mean_reward to date used ent_coef≈0.10 with clip_coef=0.3 and longer horizon (20M).
- Clip statistics show near-zero approx_kl and clipfrac in the latest run, suggesting small policy updates; increasing total timesteps and reverting to the previously stronger entropy/clip settings should allow more meaningful improvements without destabilizing PPO.
- Success remains 0.0 even after longer runs (up to 120M), so the near-term lever is to continue fine-tuning from the best checkpoint with proven settings before considering horizon/reward-shaping changes.

Next iteration plan (proposed overrides saved to proposals/next_config.json):

- Continue from best; total_timesteps=20M; ent_coef=0.10; clip_coef=0.30; update_epochs=1; keep device=mps, vec 28/56, env 4×8, bptt=16. Anneal LR and seed=42 for reproducibility.

Follow-ups if success_rate stays flat:

- Try longer horizon (bptt=32) and/or gamma=0.995 to emphasize longer returns.
- Consider modestly raising env.num_drones to 12–16 to increase batch (watch MPS VRAM and SPS). 
- If PPO updates remain tiny (clipfrac≈0), try update_epochs=2–4 paired with clip_coef=0.2.
