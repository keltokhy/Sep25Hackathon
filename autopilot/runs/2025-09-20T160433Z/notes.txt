Run 2025-09-20T160433Z — full baseline, tuned entropy down

Summary
- Steps: 11.04M agent steps (epoch 349)
- SPS: 126.3K
- Reward: mean_reward 496.56
- Success: success_rate 1258.92 (env-specific unit)
- Collisions: collision_rate 0.0050
- Episode length: 882.86
- Utilisation (from train.log panel): CPU ~357%, GPU (MPS) ~0%

Config (effective)
- train: lr 0.0025, ent_coef 0.01, bptt 16, total_timesteps 1e7, update_epochs 1
- vec/env: workers 28, vec_envs 56; env.num_envs 4; num_drones 8
- Derived batch: 4 × 8 × 56 × 16 = 28,672 (minibatch = max_minibatch = 28,672)
- Divisibility: 56 % 28 = 0 (OK)

Context & comparison (recent runs)
- 2025-09-20T155229Z (lr 0.0025, ent 0.02, 2e7 steps): mean_reward 479.79, collision_rate 0.00346, SPS 123.9K.
- 2025-09-20T154105Z (lr 0.003, ent 0.10, 2e7): mean_reward -4.95 (collapsed), collision_rate 0.00045.
- 2025-09-20T153515Z (lr 0.003, ent 0.12, 1e7): mean_reward 7.43, collision_rate 0.00073.

Takeaways
- Dropping entropy from 0.10–0.12 to 0.02 then 0.01 unlocked learning and lifted reward substantially.
- Today’s further drop to 0.01 improved mean_reward (+3.5% vs. 0.02) but slightly increased collision_rate (+0.0016 absolute).
- Approx KL and clipfrac were ~0.0 by the end, suggesting very conservative updates with current clip/update settings.
- Throughput and batch math look healthy; topology remains within guidance and divisible.

Theory
- With exploration reduced (ent=0.01), the policy is exploiting well but may be making sharper moves that increase collisions. Smaller learning rate and a second PPO epoch should refine updates and reduce jitter without sacrificing reward. Keeping clip as-is avoids too-many-moving-parts this iteration.

Next step (staged in proposals/next_config.json)
- train.learning_rate: 0.0020 (from 0.0025) — stability; aim to lower collisions.
- train.ent_coef: 0.01 (hold steady after gains).
- train.update_epochs: 2 (from 1) — slightly more refinement per batch.
- train.total_timesteps: 2e7 — validate gains at full length.
- Topology unchanged; batch stays 28,672 to preserve stability.
- Warm start: resume_mode=continue, resume_from=best, save_strategy=best.

Plan after next run
- If reward holds and collision_rate falls, consider nudging clip_coef up (e.g., 0.15) to speed convergence, or tuning vf_coef/max_grad_norm for smoother value updates. If collisions persist, test ent_coef 0.008 and/or max_grad_norm 1.0.
