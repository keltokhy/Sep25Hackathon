Run 2025-09-20T140103Z — full baseline (4×8 env, vec 28/56, bptt 16)

Summary
- Steps: 11.04M (epoch 349); SPS: 106.4K
- Device: mps; panel CPU ~357%; GPU panel ~0% (likely under-reported on MPS)
- Success rate: 0.000
- Mean reward: 2.615
- Collision rate: 0.000596
- Avg episode length: ~32.0

Checks
- Divisibility: vec.num_envs 56 % vec.num_workers 28 == 0 ✅
- Batch math: 4 envs × 8 drones × 56 vec × 16 bptt = 28,672; minibatch=max=28,672 ✅

Observations
- KL ~0 and clipfrac ~0 throughout; policy updates are tiny.
- Entropy high (≈6.39) and annealed LR likely decayed by the end.
- Very low collisions but high OOB (~0.97) → agents not stabilizing in-task.
- Throughput strong; utilization panel suggests headroom to push learning rather than concurrency.

Theory
- With update_epochs=1 and ent_coef=0.12, exploration is high while the optimizer takes too few steps per rollout. This likely yields under-updated policies (near-zero KL) and low success.

Next changes (for the NEXT run)
- Reduce entropy to focus policy: ent_coef 0.12 → 0.08.
- Increase learning per batch: update_epochs 1 → 3.
- Allow larger policy moves: clip_coef 0.2 → 0.3.
- Keep LR near baseline with a modest nudge: 0.003 → 0.0032; keep anneal_lr=true.
- Warm start from best checkpoint to retain any useful features.

Expectation
- Target >0 success_rate with similar low collision_rate.
- KL and clipfrac should move off ~0; watch for stability (approx_kl, value_loss) and SPS.

Proposed overrides saved to autopilot/proposals/next_config.json and resume set to best.
