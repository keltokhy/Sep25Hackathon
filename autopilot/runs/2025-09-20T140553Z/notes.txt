Run 2025-09-20T140553Z — full baseline; timed to completion under our 20m window.

Summary
- success_rate: 0.000
- mean_reward: 14.323
- collision_rate: 0.00058
- episode_length: 36.78
- SPS: 126.3k; agent_steps: 121.1M; epoch: 4186
- Utilization (panel): CPU ~357%, GPU 0% (MPS target; VRAM 0%)

Config & derived batch
- train.device mps; seed 42; lr 3e-3; ent_coef 0.12; update_epochs 1; bptt 16; total_timesteps 1.2e8
- vec 28/56; env 4×8 → batch = 4 × 8 × 56 × 16 = 28,672
- Divisibility check: 56 % 28 == 0 ✔️

Trend vs recent runs
- 2025-09-20T140103Z (quick): mean_reward 2.61, SPS 106k, success 0.0
- This run (full): mean_reward up to 14.32, SPS 126k, success still 0.0, collisions low and steady.
- OOB in the live panel remains very high (~0.966), suggesting episodes end by leaving bounds rather than collisions.

Theory
- High entropy (0.12) encourages wide exploration, which likely contributes to persistent OOB and near-zero success. With only 1 PPO update epoch, policy improvements per batch are shallow and may not consolidate useful behaviors before the next rollout.

Action for next run (proposal written to proposals/next_config.json)
- Reduce exploration: ent_coef → 0.08
- Increase learning per batch: update_epochs → 4
- Keep topology and bptt fixed to isolate effects; derived batch remains 28,672.
- Warm start from latest checkpoint to preserve learned progress.

Expected outcome
- Longer average episode lengths (fewer OOB terminations), potential uptick in success proxies (e.g., score) and eventual non-zero success_rate, while keeping collision_rate low.
- SPS may dip slightly due to extra update epochs, but within acceptable range on M3 Ultra.

Next checks
- Monitor OOB, episode_length, and collision_rate in the panel; verify SPS/util stays stable ≥100k.
- If success remains 0, consider: (a) lowering ent_coef further (0.06), (b) modest lr reduction (e.g., 0.0025) or (c) increasing env.num_envs to 8 (batch 57,344) if CPU headroom remains.
