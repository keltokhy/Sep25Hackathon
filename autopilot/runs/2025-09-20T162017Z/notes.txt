Run 2025-09-20T162017Z — full baseline, warm-started (resume=continue from best)

Config + topology
- Device: mps | Optimizer: muon | anneal_lr: true | seed: 42
- PPO: lr 2.5e-3, ent 0.12, update_epochs 3, clip 0.10, gae_lambda 0.95, gamma 0.99, vf_coef 2.0, vf_clip 1.0, max_grad_norm 1.0
- Topology: vec 28/56, env 4×8 (env.num_envs=4, num_drones=8)
- Derived batch: 4 × 8 × 56 × 16 = 28672 (minibatch=max=28672)
- Divisibility: 56 % 28 == 0 (OK)

Outcomes
- SPS: 126.7k | Agent steps: 20.93M | Epoch: 698 | Uptime ~3m49s
- success_rate: 9.38 | mean_reward: 250.57 | episode_length: 631.42
- collision_rate: 0.00235 (low)
- Diagnostics: clipfrac ≈ 0.0, KL ≈ 0.0 → under-updating; CPU ~357% (dashboard), GPU/VRAM ~0%

Comparison to recent full runs
- 2025-09-20T161602Z (prev, resume=best): mean_reward 474.92, collision 0.00640, SPS 132k; also showed clipfrac≈0 and KL≈0.
- This run decreased reward but improved safety (lower collision), coincident with raising entropy (0.12 vs 0.01 previously) and extending to 20M steps.

Theory
- High entropy (0.12) increased exploration, which reduced exploitation quality (lower mean_reward) but improved safety (lower collision). Persistently near-zero clipfrac and KL suggest updates are too conservative (small clip + few epochs), so policy isn’t moving enough despite long training.

Next step (staged in proposals/next_config.json)
- Reduce exploration; increase effective update size while keeping stability:
  - ent_coef: 0.06 (down from 0.12)
  - clip_coef: 0.20 (up from 0.10)
  - update_epochs: 4 (up from 3)
  - keep lr: 2.5e-3 with anneal_lr=true (to avoid vanishingly small steps)
  - keep topology (28/56, 4×8) and batch (28672) for comparability
- Warm-start policy: resume=continue from best; save_strategy=best

What to watch next
- clipfrac should rise above ~0; KL should become >0 but remain stable
- Expect mean_reward to recover while keeping collision_rate low; monitor SPS/utilization remains ~120–130k and CPU ≥ 300%
