OVERVIEW & GOALS
- Use behavioral analysis to improve the drone pick‑and‑place task (grip → carry → deliver) by changing environment code, not hyperparameters.
- Honor a strict no‑hyperparameter‑changes policy. Treat `autopilot/journal/notes.md` as long‑term memory and constraints.

THE TASK
Drone must: take off → approach box → hover → descend → grip → carry to drop zone → deliver.
Success requires completing the full chain.

KEY METRICS
- `perfect_grip` / `grip_success`: Successfully gripped box (0-1)
- `perfect_deliv` / `delivery_success`: Successfully delivered (0-1)
- `ho_pickup`: "Hovering for pickup" - drone above box before descent (0-1)
- `de_pickup`: "Descending for pickup" - drone descending toward box (0-1)
- `to_pickup`: "Toward pickup" - moving toward box (0-1)
- `to_drop`: "Toward drop" - carrying box to drop zone (0-1)
- `ho_drop`: "Hovering for drop" - above drop zone (0-1)
- `attempt_grip` / `attempt_drop`: Number of attempts
- `oob`: Out of bounds rate (0-1)
- `collision_rate`: Hit obstacles/floor (0-1)
- `episode_return`: Total reward (higher better)

CONTEXT
- Iteration: {iteration}
- Run ID: {run_id}

OPERATING PRINCIPLES
- Consult `autopilot/journal/notes.md` before acting, especially "Header evolution: `drone_pp.h`" section.
- State behavioral hypothesis and expected metrics impact for each change.
- If no improvement after few iterations, try different dimension.
- If performance degrades at consistent training points, suspect curriculum (k parameter) issues - consider making configurable.
- Track which 'dimensions' tried (gates/physics/rewards/spawn/curriculum) and explore untouched ones.

SINGLE‑RUN PER ITERATION
- Launch training once per iteration. Edits apply to next iteration's run. No auto-retry on failures.

WORKFLOW (Numbered Core Loop)
1) Execute {script} (autopilot/scripts/run_training.sh) with ≥15‑minute timeout. Do not skip.
2) Read the fresh run's `summary.json` for behavioral_analysis (note: this field is not printed in train.log "User Stats"). Use `trainer_summary.json` or `train.log` for raw metrics.
3) Identify the specific failure mode from behavioral_analysis and metrics. Cross‑check against the "Header evolution: `drone_pp.h`" notes to avoid re‑introducing patterns previously removed.
4) Make minimal, targeted environment code changes to address the failure, aligned with the historical intent captured in Notes. Maintain hypothesis diversity: Keep a list of several different approaches. Don't repeatedly test the same hypothesis type (e.g., "relax gates") without trying something orthogonal.
5) Rebuild bindings and smoke‑test import.
6) Configure the next run using only `autopilot.*` overrides (or `{}`) in `autopilot/proposals/next_config.json`.
7) Record actions/observations/outcomes/next steps in labbook and notes; repeat.

DECISION FRAMEWORK
• diagnostic_grip (when perfect_grip = 0):
  - If ho_pickup < 0.1: Drone can't reach hover → relax hover gates or adjust spawn
  - If ho_pickup > 0.1 but de_pickup < 0.1: Hover works but won't descend → check descent gates
  - If de_pickup > 0.1 but perfect_grip = 0: Descending but can't grip → relax grip conditions
  - If metrics improve then degrade at consistent epoch: Curriculum too aggressive → control k decay

• improve_carrying (when perfect_grip > 0 but perfect_deliv = 0):
  - Check to_drop: If low, navigation issue → adjust carry rewards
  - Check ho_drop: If low, can't stabilize at drop → tune drop hover gates

• fix_stability (when oob > 0.5 or collision_rate > 0.1):
  - High OOB often means spawn too close to boundaries or descent too aggressive

CREATIVE ESCALATION (When progress stalls)
- Early iterations: Focus on PRIORITY 1 (.ini tweaks) and PRIORITY 2 (drone_pp.h gates/logic)
- Mid iterations: If limited progress, try different dimension in same priority level
- Later iterations: Escalate to PRIORITY 3 (observation space, new metrics)
- After several iterations with minimal improvement, try orthogonal approaches:
  • Reward shaping: Modify reward weights in .ini first, then computation in .h
  • Physics tuning: Adjust box mass, drone thrust, gravity constants
  • Curriculum control: Make k decay configurable via .ini (check if k calculation in .h could read from config)
  • Anti-collapse mechanisms: Add exploration bonuses or success-gated progression
  • Spawn strategy: Alter initial positions/orientations fundamentally

WHAT YOU CAN CHANGE
- Environment code:
  • `drone_pp.h` - Main logic, states, physics, rewards
  • `drone_pp.c` - Core simulation
  • `drone_pp.py` - Observation/action spaces
  • `binding.c` - Python-C interface
  • `dronelib.h` - Physics utilities
  • `drone_pp.ini` - ALL environment parameters including rewards, episode_length, and ANY NEW PARAMETERS YOU ADD (like k_decay_rate, k_start, k_end)
- Autopilot overrides (`autopilot/proposals/next_config.json`):
  • ONLY: resume_mode, resume_from, save_strategy

WHAT YOU CANNOT CHANGE
- Training hyperparameters in proposals (`train.*`, `env.*`, `vec.*`)
- NOTE: drone_pp.ini is NOT a hyperparameter file - it's environment configuration you CAN modify freely

ENVIRONMENT DEBUGGING CHECKLIST
1) Identify failure from behavioral_analysis and concrete metrics.
2) Consult `autopilot/journal/notes.md` → "Header evolution: `drone_pp.h` (Sep 11 → Sep 20, 2025)" to understand prior changes and avoid reversions of removed behaviors (e.g., low‑altitude penalty) unless explicitly justified.
3) Choose the right file to edit (in priority order):
   PRIORITY 1 (Try first - no rebuild needed):
   • `drone_pp.ini` - You can ADD ANY parameters here (rewards, k_decay, k_start, etc.)
     Then read them in drone_pp.h: env->k_decay_rate = get_config_float("k_decay_rate", 0.1)
     Check if difficulty/curriculum parameters exist or could be externalized here

   PRIORITY 2 (Most impactful - requires rebuild):
   • `drone_pp.h` - Main logic, state machines, reward computation, physics
     Consider: Environment-side solutions to training dynamics issues

   PRIORITY 3 (When deeper changes needed):
   • `drone_pp.py` - To change observation/action spaces
   • `binding.c` - To expose new metrics for diagnostics

   PRIORITY 4 (Rarely needed):
   • `drone_pp.c` - Core simulation loop
   • `dronelib.h` - Fundamental physics utilities
4) Make a minimal, documented change tied to the hypothesis and historical context.
5) Rebuild: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`.
6) Test: `pytest PufferLib/tests/test_env_binding.py -q` (if present) and ensure import works.
7) Document in `runs/<run_id>/notes.txt`: problem → change → expected outcome; reference the specific historical bullet(s) you followed or intentionally deviated from.

SUCCESS METRICS
- Baseline "working" when:
  • grip_success shows majority success
  • delivery_success shows significant rate
  • end_to_end_success shows upward trend
  • OOB/crash rates remain low and stable
- Intermediate milestones (reassess if not met):
  • Early: ho_pickup showing improvement
  • Mid: de_pickup increasing
  • Later: perfect_grip becoming non-zero
  • Eventually: perfect_grip showing consistent growth
- If milestones stagnate, switch approach dimension
- Note: Consistent degradation at same epoch suggests curriculum issues (k parameter)

FAILSAFES
- No behavioral_analysis: Check `summary.json` (not in train.log), add logging if needed
- Build fails: Revert patch, rebuild clean
- Metrics regress multiple runs: Revert to known-good state
- Prefer smaller edits over large refactors

TECHNICAL DETAILS
- Build: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`
- Tests: `pytest PufferLib/tests/test_env_binding.py -q` when available
- Constraints:
  • vec.num_envs % vec.num_workers == 0
  • train.batch_size = env.num_envs × env.num_drones × vec.num_envs × train.bptt_horizon
- Device: Prefer `mps`; use `cpu` only for diagnostics
- Run artifacts: `$PUFFER_AUTOPILOT_RUN_DIR`; logs under `autopilot/logs/`

EXECUTION RULES
- Step 1 is mandatory: Launch {script} immediately from repo root and allow ≥15 minutes. Wait until it completes and capture stdout/stderr. Diagnose early exits before changing configs.
- Run training only once per iteration. Do not start a second training after making edits; those edits are for the next iteration's single run.
- After the run, write the next overrides as `{}` or only `autopilot.*` keys. Do not propose any other hparam changes.

RECORDKEEPING
- Labbook (`autopilot/journal/labbook.md`): actions, observations, outcomes, next steps
- Notes (`autopilot/journal/notes.md`): curated long-term memory, edit in place, keep concise
- Run directories: `autopilot/runs/<run_id>` where `<run_id>` = `YYYY-MM-DDTHHMMSSZ`
- Always reference exact run_id in commits and notes

COMMIT DISCIPLINE (MANDATORY)
- After EVERY single training run, make exactly one commit capturing:
  • Environment code edits
  • Run artifacts under `autopilot/runs/<run_id>/`
  • Journal updates (labbook.md and notes.md)
  • `autopilot/proposals/next_config.json` for next run
- ALWAYS immediately push to current branch. Never skip the push.
- Commit message format: `env(drone_pp): <change> | <run_id>`
  Follow with bullets: problem, change, expected, outcome, next

ENVIRONMENT CHANGE TRACKING
- Each run captures diffs and environment comparison
- Correlate changes with outcomes to decide keep/revert
- Document changes and impacts in run notes

NOTES.MD DISCIPLINE
- Edit in place, keep ≤150 lines, prefer bullets
- Structure:
  1) Current Baseline
  2) Stable Learnings (5-10 bullets)
  3) Header Evolution
  4) Open Questions & Next Hypotheses (≤5 bullets)
  5) Decisions Log (dated bullets with run IDs)
  6) Behavioral Patterns (what worked AND what didn't)
- Division: detailed narratives in labbook.md; notes.md for quick reference
