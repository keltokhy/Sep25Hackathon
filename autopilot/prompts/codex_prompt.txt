OVERVIEW & GOALS
- Use behavioral analysis to improve the drone pick‑and‑place task (grip → carry → deliver) by changing environment code, not hyperparameters.
- Honor a strict no‑hyperparameter‑changes policy. Treat `autopilot/journal/notes.md` as long‑term memory and constraints.

THE TASK
Drone must: take off → approach box → hover → descend → grip → carry to drop zone → deliver.
Success requires completing the full chain.

KEY METRICS
- `perfect_grip` / `grip_success`: Successfully gripped box (0-1)
- `perfect_deliv` / `delivery_success`: Successfully delivered (0-1)
- `ho_pickup`: "Hovering for pickup" - drone above box before descent (0-1)
- `de_pickup`: "Descending for pickup" - drone descending toward box (0-1)
- `to_pickup`: "Toward pickup" - moving toward box (0-1)
- `to_drop`: "Toward drop" - carrying box to drop zone (0-1)
- `ho_drop`: "Hovering for drop" - above drop zone (0-1)
- `attempt_grip` / `attempt_drop`: Number of attempts
- `oob`: Out of bounds rate (0-1)
- `collision_rate`: Hit obstacles/floor (0-1)
- `episode_return`: Total reward (higher better)

CONTEXT
- Iteration: {iteration}
- Run ID: {run_id}

OPERATING PRINCIPLES
- First understand WHY before changing WHAT. Trace mathematical relationships and actual values in the code.
- When OOB is 95%, that's the MAIN problem - everything else is secondary until OOB is fixed.
- Physics modifications often make things WORSE: excessive drag makes control sluggish, soft walls fight control.
- Check actual grid boundaries (GRID_X/Y/Z in dronelib.h) and spawn positions to understand OOB causes.
- Consult `autopilot/journal/notes.md` before acting, especially "Header evolution: `drone_pp.h`" section.
- When metrics worsen after a change, REVERT harmful additions rather than layering more fixes.
- Multiple physics "helpers" (soft walls + centralizing + high drag) often fight each other.
- State behavioral hypothesis and expected metrics impact for each change.
- If no improvement after few iterations, try different dimension.
- If performance degrades at consistent training points, suspect curriculum (k parameter) issues.
- Track which 'dimensions' tried (gates/physics/rewards/spawn/curriculum) and explore untouched ones.
- Before adding new mechanisms, check if existing parameters in .ini/.c files can achieve the goal.

SINGLE‑RUN PER ITERATION
- Launch training once per iteration. Edits apply to next iteration's run. No auto-retry on failures.

WORKFLOW (Numbered Core Loop)
1) Execute {script} (autopilot/scripts/run_training.sh) with ≥15‑minute timeout. Do not skip.
2) Read the fresh run's `summary.json` for behavioral_analysis (note: this field is not printed in train.log "User Stats"). Use `trainer_summary.json` or `train.log` for raw metrics.
3) Analyze: Compare metrics across last 3 runs. Are changes helping or hurting? Look for patterns in timing (which epoch fails). Also examine score evolution within each run - does score improve then collapse at a specific epoch? This often reveals curriculum or hot-patch issues.
4) Every 5th iteration: Launch meta-reviewer with `codex exec < autopilot/prompts/meta_reviewer_prompt.txt` for systemic analysis
5) Root cause: Identify the specific failure mode from behavioral_analysis and metrics. Ask: is this a symptom or the cause? Trace back through the mathematical formulas.
6) Solution: Make minimal, targeted environment code changes to address the root cause, aligned with the historical intent captured in Notes. If recent changes made things worse, revert them first.
7) Rebuild bindings and smoke‑test import if .h/.c files changed.
8) Configure the next run using only `autopilot.*` overrides (or `{}`) in `autopilot/proposals/next_config.json`.
9) Record actions/observations/outcomes/next steps in labbook and notes; repeat.

DECISION FRAMEWORK
• diagnostic_grip (when perfect_grip = 0):
  - If ho_pickup < 0.1: Drone can't reach hover → relax hover gates or adjust spawn
  - If ho_pickup > 0.1 but de_pickup < 0.1: Hover works but won't descend → check descent gates
  - If de_pickup > 0.1 but perfect_grip = 0: Descending but can't grip → relax grip conditions
  - If metrics improve then degrade at consistent epoch: Curriculum too aggressive → reduce grip_k_decay in drone_pp.ini

• improve_carrying (when perfect_grip > 0 but perfect_deliv = 0):
  - Check to_drop: If low, navigation issue → adjust carry rewards
  - Check ho_drop: If low, can't stabilize at drop → tune drop hover gates

• fix_stability (when oob > 0.5 or collision_rate > 0.1):
  - High OOB often means spawn too close to boundaries or descent too aggressive

CREATIVE ESCALATION (When progress stalls)
- First ask: Have recent changes made metrics WORSE? If yes, revert harmful additions before trying new things.
- REVERT CHECKLIST when OOB > 0.90:
  • Check BASE_B_DRAG in dronelib.h (should be ~0.1, not 0.5)
  • Check BASE_K_ANG_DAMP in dronelib.h (should be ~0.2, not 0.35)
  • Remove soft walls/floors/ceilings if present
  • Remove centralizing fields if present
  • These physics "helpers" often make control worse!
- Early iterations: Focus on PRIORITY 1 (.ini/.c tweaks) - often the simplest parameter adjustment solves the issue
- Mid iterations: If limited progress, try different dimension in same priority level
- Later iterations: Escalate to PRIORITY 2/3 only if .ini/.c changes insufficient
- After several iterations with minimal improvement, try orthogonal approaches:
  • Parameter tuning: Check if grip_k_decay or other rates need adjustment
  • Reward shaping: Modify reward weights in .ini first, then computation in .h
  • Physics tuning: Adjust box mass, drone thrust - but AVOID excessive damping
  • Curriculum control: grip_k_decay in drone_pp.c controls difficulty progression speed
  • Spawn strategy: Alter initial positions/orientations fundamentally

WHAT YOU CAN CHANGE
- Environment code:
  • `drone_pp.h` - Main logic, states, physics, rewards
  • `drone_pp.c` - Core simulation
  • `drone_pp.py` - Observation/action spaces
  • `binding.c` - Python-C interface
  • `dronelib.h` - Physics utilities
  • `drone_pp.ini` - ALL environment parameters including rewards, episode_length, and ANY NEW PARAMETERS YOU ADD (like k_decay_rate, k_start, k_end)
- Autopilot tools and helpers:
  • You can modify your own scripts in `autopilot/scripts/` to add diagnostics or analysis
  • You can create helper functions in `autopilot/utils/` for metric analysis
  • You can enhance logging/debugging in the training pipeline
- Autopilot overrides (`autopilot/proposals/next_config.json`):
  • ONLY: resume_mode, resume_from, save_strategy

WHAT YOU CANNOT CHANGE
- Training hyperparameters in proposals (`train.*`, `env.*`, `vec.*`)
- NOTE: drone_pp.ini is NOT a hyperparameter file - it's environment configuration you CAN modify freely

ENVIRONMENT DEBUGGING CHECKLIST
1) Identify failure from behavioral_analysis and concrete metrics.
2) Consult `autopilot/journal/notes.md` → "Header evolution: `drone_pp.h` (Sep 11 → Sep 20, 2025)" to understand prior changes and avoid reversions of removed behaviors (e.g., low‑altitude penalty) unless explicitly justified.
3) Choose the right file to edit (in priority order):
   PRIORITY 1 (Try first - no rebuild needed):
   • `drone_pp.ini` - Modify existing parameters like grip_k_decay (currently 0.09049941256843744)
     WARNING: Check if parameters are actually read from config! Some are hardcoded in drone_pp.c
     This controls how fast difficulty decreases: env->grip_k = tick * -grip_k_decay + grip_k_max
     Lower values = slower decay = more time to learn at each difficulty level
     Math insight: With current values, k drops from 17.9 to 1.0 in ~200k steps. Is that too fast?

   PRIORITY 2 (Most impactful - requires rebuild):
   • `drone_pp.h` - Main logic, state machines, reward computation, physics
     Consider: Environment-side solutions to training dynamics issues
   • `drone_pp.c` - CRITICAL: Check line 137-143 for hardcoded parameters that override configs!
     Many values like grip_k_decay are hardcoded here and ignore .ini files

   PRIORITY 3 (When deeper changes needed):
   • `drone_pp.py` - To change observation/action spaces
   • `binding.c` - To expose new metrics for diagnostics

   PRIORITY 4 (Rarely needed):
   • `dronelib.h` - Fundamental physics utilities
4) Make a minimal, documented change tied to the hypothesis and historical context.
5) Rebuild: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`.
6) Test: `pytest PufferLib/tests/test_env_binding.py -q` (if present) and ensure import works.
7) Document in `runs/<run_id>/notes.txt`: problem → change → expected outcome; reference the specific historical bullet(s) you followed or intentionally deviated from.

SUCCESS METRICS
- CRITICAL: If OOB > 0.90, this is the PRIMARY problem - fix this before anything else
- WARNING SIGN: If OOB increases from ~0.85 to >0.95 after a change, that change is harmful - revert it
- TRACK SCORE PATTERNS: Score is a composite metric - watch if it improves then suddenly drops (e.g., epoch 64→65). This often indicates mid-training code changes or curriculum collapse
- Check dronelib.h for actual physics constants: BASE_B_DRAG should be ~0.1, not 0.5!
- Baseline "working" when:
  • grip_success shows majority success
  • delivery_success shows significant rate
  • end_to_end_success shows upward trend
  • OOB/crash rates remain low and stable
- Intermediate milestones (reassess if not met):
  • Early: ho_pickup showing improvement (>0.01 and rising)
  • Mid: de_pickup increasing
  • Later: perfect_grip becoming non-zero
  • Eventually: perfect_grip showing consistent growth
- If milestones stagnate or reverse, first check if recent changes are harmful
- Note: Consistent degradation at same epoch suggests curriculum issues (grip_k_decay parameter)

FAILSAFES
- No behavioral_analysis: Check `summary.json` (not in train.log), add logging if needed
- Build fails: Revert patch, rebuild clean
- Metrics regress multiple runs: Revert to known-good state
- Prefer smaller edits over large refactors

TECHNICAL DETAILS
- Build: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`
- Tests: `pytest PufferLib/tests/test_env_binding.py -q` when available
- Constraints:
  • vec.num_envs % vec.num_workers == 0
  • train.batch_size = env.num_envs × env.num_drones × vec.num_envs × train.bptt_horizon
- Device: Prefer `mps`; use `cpu` only for diagnostics
- Run artifacts: `$PUFFER_AUTOPILOT_RUN_DIR`; logs under `autopilot/logs/`

EXECUTION RULES
- Step 1 is mandatory: Launch {script} immediately from repo root and allow ≥15 minutes. Wait until it completes and capture stdout/stderr. Diagnose early exits before changing configs.
- Run training only once per iteration. Do not start a second training after making edits; those edits are for the next iteration's single run.
- After the run, write the next overrides as `{}` or only `autopilot.*` keys. Do not propose any other hparam changes.

RECORDKEEPING
- Labbook (`autopilot/journal/labbook.md`): actions, observations, outcomes, next steps
- Notes (`autopilot/journal/notes.md`): curated long-term memory, edit in place, keep concise
- Run directories: `autopilot/runs/<run_id>` where `<run_id>` = `YYYY-MM-DDTHHMMSSZ`
- Always reference exact run_id in commits and notes

COMMIT DISCIPLINE (MANDATORY)
- After EVERY single training run, make exactly one commit capturing:
  • Environment code edits
  • Run artifacts under `autopilot/runs/<run_id>/`
  • Journal updates (labbook.md and notes.md)
  • `autopilot/proposals/next_config.json` for next run
- ALWAYS immediately push to current branch. Never skip the push.
- Commit message format: `env(drone_pp): <change> | <run_id>`
  Follow with bullets: problem, change, expected, outcome, next

ENVIRONMENT CHANGE TRACKING
- Each run captures diffs and environment comparison
- Correlate changes with outcomes to decide keep/revert
- Document changes and impacts in run notes

NOTES.MD DISCIPLINE
- Edit in place, keep ≤150 lines, prefer bullets
- Structure:
  1) Current Baseline
  2) Stable Learnings (5-10 bullets)
  3) Header Evolution
  4) Open Questions & Next Hypotheses (≤5 bullets)
  5) Decisions Log (dated bullets with run IDs)
  6) Behavioral Patterns (what worked AND what didn't)
- Division: detailed narratives in labbook.md; notes.md for quick reference
