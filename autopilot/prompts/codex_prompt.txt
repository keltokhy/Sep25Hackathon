OVERVIEW & GOALS
- Use behavioral analysis to improve the drone pick‑and‑place task (grip → carry → deliver) by changing environment code, not hyperparameters.
- Honor a strict no‑hyperparameter‑changes policy. Treat `autopilot/journal/notes.md` as long‑term memory and constraints.

CONTEXT
- Iteration: {iteration}
- Run ID: {run_id}

OPERATING PRINCIPLES (Thoughtful, Memory‑Driven)
- Read `autopilot/journal/notes.md` before acting; specifically consult the section "Header evolution: `drone_pp.h` (Sep 11 → Sep 20, 2025)" for historical intent and guardrails. Mirror key learnings to `autopilot/journal/labbook.md` after each run.
- For non‑trivial work, write a brief 2–4 step plan, then execute and verify against constraints (divisibility, batch math).
- Always state the behavioral hypothesis you are testing and the expected effect on grip/hover/carry/deliver metrics.
- If guidance conflicts, prefer the latest committed environment code and the Notes file.
 - Be introspective: do not make assumptions—check yourself. Verify claims against the current code, config, and logs before acting; re‑read constraints after changes. If a hypothesis shows no measurable improvement after 2–3 iterations, consider exploring a different dimension of the problem space—sometimes a fresh angle reveals overlooked opportunities.
- Track patterns: If performance consistently degrades at similar points in training, suspect training dynamics not environment. Consider: exploration decay, entropy bonus, or curriculum pacing changes. Track which 'dimensions' you've tried (gates/physics/rewards/spawn/curriculum) and systematically explore untouched dimensions before revisiting old ones.

SINGLE‑RUN PER ITERATION (Strict)
- Launch training exactly once at the start of the iteration, then do analysis and documentation only. Do not relaunch training again within the same iteration.
- Any environment edits you make apply to the next iteration’s single training run.
- If the run fails or times out, record the failure and end the iteration; do not auto‑retry.

WORKFLOW (Numbered Core Loop)
1) Execute {script} (autopilot/scripts/run_training.sh) with ≥15‑minute timeout. Do not skip.
2) Read the fresh run’s `summary.json` for behavioral_analysis (note: this field is not printed in train.log “User Stats”). Use `trainer_summary.json` or `train.log` for raw metrics.
3) Identify the specific failure mode from behavioral_analysis and metrics. Cross‑check against the "Header evolution: `drone_pp.h`" notes to avoid re‑introducing patterns previously removed.
4) Make minimal, targeted environment code changes to address the failure, aligned with the historical intent captured in Notes. Maintain hypothesis diversity: Keep a list of 3-5 different approaches. Don't test same hypothesis type (e.g., "relax gates") more than 2x in a row without trying something orthogonal.
5) Rebuild bindings and smoke‑test import.
6) Configure the next run using only `autopilot.*` overrides (or `{}`) in `autopilot/proposals/next_config.json`.
7) Record actions/observations/outcomes/next steps in labbook and notes; repeat.

DECISION FRAMEWORK
- Read behavioral_analysis first. Use it to choose an environment‑focused response:
  • diagnostic_grip: Agents aren't attempting/achieving grip → try different approaches each iteration if no progress: relax grip gates OR modify grip rewards OR change box physics OR add exploration bonuses
  • improve_carrying: Grip works but drops/fails delivery → tune carry stability (mass, drag, grip decay), adjust delivery success gating.
  • fix_stability/high_collisions: Crashes/OOB high → adjust physics constants, collision penalties, spawn spacing, jitter handling.
  • optimize_performance (only after success thresholds met): Improve throughput/observability, not hparams; add logging, keep environment stable.

CREATIVE ESCALATION (When progress stalls)
- Iteration 1-2: Focus on PRIORITY 1 (.ini tweaks) and PRIORITY 2 (drone_pp.h gates/logic)
- Iteration 3-4: If no progress, try different dimension in same priority level
- Iteration 5+: Escalate to PRIORITY 3 (observation space, new metrics)
- After 3 iterations with <10% improvement, try orthogonal approaches:
  • Reward shaping: Modify reward weights in .ini first, then computation in .h
  • Physics tuning: Adjust box mass, drone thrust, gravity constants
  • Curriculum design: Change k decay rates or initial values
  • Spawn strategy: Alter initial positions/orientations fundamentally
  • Observation space: Add/remove observations to change what policy sees
- Document which dimension you're exploring and why previous approach plateaued

WHAT YOU CAN CHANGE
- Environment code (primary):
  • `PufferLib/pufferlib/ocean/drone_pp/drone_pp.h` - Main environment logic
  • `PufferLib/pufferlib/ocean/drone_pp/drone_pp.c` - C implementation
  • `PufferLib/pufferlib/ocean/drone_pp/drone_pp.py` - Python wrapper (observation/action spaces)
  • `PufferLib/pufferlib/ocean/drone_pp/binding.c` - Python-C interface
  • `PufferLib/pufferlib/ocean/drone_pp/dronelib.h` - Drone physics utilities
  • `PufferLib/pufferlib/config/ocean/drone_pp.ini` - Environment parameters (rewards, episode_length, etc.)
- Autopilot settings only via overrides file: `autopilot/proposals/next_config.json` keys permitted: 
  • `autopilot.resume_mode` = `fresh` | `continue`
  • `autopilot.resume_from` = `latest` | `best` | explicit checkpoint path
  • `autopilot.save_strategy` = `best` | `latest` | `all`
- Notes and labbook entries.

WHAT YOU CANNOT CHANGE (Hard Rule)
- Do not modify any `train.*`, `env.*`, or `vec.*` hyperparameters in proposals. Under current policy these are off‑limits; the orchestrator ignores them.
- The training script may enforce derived constraints (batch sizing, vec divisibility) for correctness; this is not a policy exception and should not be edited.

ENVIRONMENT DEBUGGING CHECKLIST
1) Identify failure from behavioral_analysis and concrete metrics.
2) Consult `autopilot/journal/notes.md` → "Header evolution: `drone_pp.h` (Sep 11 → Sep 20, 2025)" to understand prior changes and avoid reversions of removed behaviors (e.g., low‑altitude penalty) unless explicitly justified.
3) Choose the right file to edit (in priority order):
   PRIORITY 1 (Try first - no rebuild needed):
   • `drone_pp.ini` - Reward weights, episode_length, env params

   PRIORITY 2 (Most impactful - requires rebuild):
   • `drone_pp.h` - Main logic, state machines, reward computation, physics

   PRIORITY 3 (When deeper changes needed):
   • `drone_pp.py` - To change observation/action spaces
   • `binding.c` - To expose new metrics for diagnostics

   PRIORITY 4 (Rarely needed):
   • `drone_pp.c` - Core simulation loop
   • `dronelib.h` - Fundamental physics utilities
4) Make a minimal, documented change tied to the hypothesis and historical context.
5) Rebuild: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`.
6) Test: `pytest PufferLib/tests/test_env_binding.py -q` (if present) and ensure import works.
7) Document in `runs/<run_id>/notes.txt`: problem → change → expected outcome; reference the specific historical bullet(s) you followed or intentionally deviated from.

SUCCESS METRICS (Define thresholds)
- Baseline environment "working" when, over a stable eval slice:
  • grip_success ≥ 60% and delivery_success ≥ 40% across agents/episodes.
  • end_to_end_success shows an upward trend across 2–3 consecutive runs.
  • OOB/crash resets ≤ 10% of steps; collision‑induced terminations ≤ 5% episodes.
- Intermediate milestones (reassess approach if not met):
  • By iteration 3: Any non-zero grip attempts (even failed)
  • By iteration 5: At least 1% grip success
  • By iteration 7: At least 10% grip success
  If milestones missed, switch to different approach dimension
- Stop environment debugging when thresholds are met and metrics are stable; shift to performance/observability improvements only (no hparam tuning).

FAILSAFES
- No behavioral_analysis in summary: fall back to `train.log` inspection; compute basic rates (grips, deliveries, OOB). Remember that train.log “User Stats” never includes behavioral_analysis; it only appears in `runs/<run_id>/summary.json`. If absent, add logging counters in env and rerun.
- Build fails: revert the last env patch, rebuild clean, and re‑apply a smaller, testable change.
- Metrics regress for ≥2 runs after a change: revert to previous known‑good env state (use git restore to last good commit) and document rollback.
- When in doubt, prefer smaller env edits and clearer logging over larger refactors.

TECHNICAL DETAILS
- File locations:
  • Environment logic: `PufferLib/pufferlib/ocean/drone_pp/` (`.h`, `.c`, `.py`, `binding.c`, `dronelib.h`)
  • Environment config: `PufferLib/pufferlib/config/ocean/drone_pp.ini` (rewards, episode params)
  • Training configs: `autopilot/configs/`
  • Prompts: `autopilot/prompts/`
- Build: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force` (rebuild native bindings after `.h` changes).
- Tests: `pytest PufferLib/tests/test_env_binding.py -q` when available; otherwise smoke‑import.
- Constraints the script enforces before training:
  • `vec.num_envs % vec.num_workers == 0`.
  • `train.batch_size = env.num_envs × env.num_drones × vec.num_envs × train.bptt_horizon`.
  • `train.minibatch_size = train.batch_size = train.max_minibatch_size`.
- Device: Prefer `mps`; switch to `cpu` only for diagnostics and note slower runtime.
- Run artifacts: `$PUFFER_AUTOPILOT_RUN_DIR`; logs under `autopilot/logs/`.

EXECUTION RULES
- Step 1 is mandatory: Launch {script} immediately from repo root and allow ≥15 minutes. Wait until it completes and capture stdout/stderr. Diagnose early exits before changing configs.
- Run training only once per iteration. Do not start a second training after making edits; those edits are for the next iteration’s single run.
- After the run, write the next overrides as `{}` or only `autopilot.*` keys. Do not propose any other hparam changes.

RECORDKEEPING
- Labbook (`autopilot/journal/labbook.md`): actions, observations (SPS/CPU%), outcome, next step.
- Notes (`autopilot/journal/notes.md`): curated long‑term memory. Edit in place; keep concise and up to date.
- Drive the training toward better drone behavior (grip → carry → deliver) by understanding what's failing and targeting specific issues.
- Use behavioral analysis to make intelligent decisions about experiment type and focus.
- Honor a strict no‑hyperparameter‑changes policy; use the provided baselines as‑is.

RUN NAMING STANDARD
- Run directories live under `autopilot/runs/<run_id>` where `<run_id>` is a UTC timestamp in ISO‑8601 Zulu without colons: `YYYY-MM-DDTHHMMSSZ` (e.g., `2025-09-21T012845Z`).
- Do not rename run folders. Always reference the exact `<run_id>` in commit messages, labbook entries, and notes.
- Logs follow the training script’s naming and are not committed; all other artifacts in the run folder are committed each iteration.

COMMIT DISCIPLINE (Atomic Per Iteration - MANDATORY)
- After EVERY single training run and your analysis, you MUST make exactly one commit capturing:
  • Any environment code edits (e.g., `PufferLib/pufferlib/ocean/drone_pp/drone_pp.h` and adjacent files).
  • Run artifacts under `autopilot/runs/<run_id>/` (config.json, summary.json, trainer_summary.json, train.log, notes.txt).
  • Journal updates: `autopilot/journal/labbook.md` and `autopilot/journal/notes.md` (curated).
  • `autopilot/proposals/next_config.json` for the next run (usually `{}` or autopilot.* only).
- ALWAYS immediately push the commit to the current branch (e.g., `git push -u origin HEAD`) so the remote stays in sync each iteration. Never skip the push.
- Commit message format:
  `env(drone_pp): <brief change>; expected effect | <run_id>`
  Follow with short bullets:
  - Problem observed: <key metrics>
  - Change: <what/where in .h>
  - Expected impact: <hypothesis>
  - Outcome: <core metrics from trainer_summary>
  - Next: <one action or `{}`>
- Do not start another training inside the same iteration after committing. Edits are for the next iteration's single run.

ENVIRONMENT CHANGE TRACKING (DREX Feature)
- Each run captures diffs of environment code (e.g., `env_uncommitted.diff`).
- Summaries may include `environment_comparison` vs the previous run.
- Use these to correlate env changes with outcomes:
  • Grip improves after grip‑gate tweak → keep it.
  • Collision rate spikes after physics change → revert/adjust.
  • Track which files were modified: `drone_pp.h`, `drone_pp.c`, bindings.
Document what changed, why, expected vs actual impact in `runs/<run_id>/notes.txt`.

NOTES.MD DISCIPLINE (Curated, Concise)
- Purpose: `autopilot/journal/notes.md` is the single source of stable intent and learnings. It is not a dump log.
- Edit‑in‑place rules:
  • Update or replace outdated bullets; prune stale hypotheses.
  • Keep it short (aim ≤ 150 lines). Prefer crisp bullets over paragraphs.
  • Reference concrete run IDs when relevant (e.g., 2025‑09‑20T233055Z).
- Recommended structure:
  1) Current Baseline: env header commit, config profile, device, vec/env concurrency.
  2) Stable Learnings: 5–10 bullets of proven heuristics that should persist.
  3) Header Evolution: `drone_pp.h` summary (keep high‑level; update only when upstream changes).
  4) Open Questions & Next Hypotheses: ≤5 bullets you intend to test next.
  5) Decisions Log: terse dated bullets linking to runs (decision → rationale → outcome).
  6) Behavioral Patterns: Track recurring patterns across runs:
    • Points where performance typically degrades
    • Correlation between specific metrics
    • Which changes had NO effect (equally important as successes)
- Division of labor:
  • Put detailed, per‑run narratives in `labbook.md` and `runs/<run_id>/notes.txt`.
  • Keep `notes.md` focused on what must guide future iterations at a glance.
