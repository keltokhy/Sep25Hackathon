Goal:
- Drive the training toward better drone behavior (grip → carry → deliver) by understanding what's failing and targeting specific issues.
- Use behavioral analysis to make intelligent decisions about experiment type and focus.

Context:
- BEHAVIORAL ANALYSIS: Each run's summary.json contains a "behavioral_analysis" field with:
  * "insights": What the drone is actually doing wrong
  * "severity": critical/warning/ok - how bad the problem is
  * "next_focus": Suggested area to improve (diagnostic_grip, improve_carrying, optimize_performance, etc.)
  * "metrics_summary": Quick view of grip/delivery/end-to-end success rates

Decision Framework for DREX:
- Check behavioral_analysis BEFORE changing hyperparameters:
  * severity="critical" → Run SHORT diagnostic experiments (100K-500K timesteps)
  * next_focus="diagnostic_grip" → Drone isn't trying to grip - fundamental issue
  * next_focus="improve_carrying" → Grip works but delivery fails - different problem
  * next_focus="optimize_performance" → Basics work - now optimize hyperparameters
- Match experiment type to problem:
  * Not attempting grip? → Reduce complexity, shorter runs, debug reward structure
  * High collision rate? → Focus on stability params, not learning rate
  * Can grip but can't deliver? → Navigation/carrying issue, not optimization

Context:
- Baseline configs live in `autopilot/configs/` (quick vs. full).
- Runtime budget guidance: quick mode uses the smoke baseline and is ideal for exploratory tweaks; full mode relies on the high-util baseline (~1e7–2e7 timesteps) and is where you validate promising theories for real gains. When proposing changes, weigh iteration speed against the chance of higher reward.
- `autopilot/proposals/next_config.json` starts empty before each run; fill it **after** the run with overrides for the next iteration. Touch only these keys and respect the safety ranges:
  * Core PPO scalars: `train.learning_rate` (1e-6–1.0), `train.ent_coef` (0–1), `train.bptt_horizon` (1–512), `train.total_timesteps` (1,000–1,000,000,000), `train.seed` (0–2,147,483,647), `train.update_epochs` (1–32), `train.gae_lambda` (0–1), `train.gamma` (0–0.999999), `train.clip_coef` (0–1), `train.vf_clip_coef` (0–10).
  * Optimiser & stability knobs: `train.optimizer` (`muon`, `adam`, or `adamw`), `train.vf_coef` (0–10), `train.max_grad_norm` (0–100), `train.checkpoint_interval` (1–1,000,000), `train.adam_beta1` (0–0.999999), `train.adam_beta2` (0–0.9999999), `train.adam_eps` (1e-14–1e-2).
  * Schedule & determinism toggles: `train.anneal_lr`, `train.torch_deterministic`, `train.cpu_offload`, `train.compile`, `train.compile_fullgraph` (Booleans; stick with JSON `true`/`false`), plus `train.precision` (`float32`, `bfloat16`) and `train.compile_mode` (documented choices in PufferLib, default `max-autotune-no-cudagraphs`).
  * Device & topology: `train.device` (`mps`, `cpu`, `cuda`), `env.num_envs`, `env.num_drones` (1–256), `vec.num_envs`, `vec.num_workers` (1–256).
  * Autopilot run policy (not CLI flags; handled by the orchestrator): `autopilot.resume_mode` (`fresh` or `continue`), `autopilot.resume_from` (`latest`, `best`, or an explicit checkpoint path), and `autopilot.save_strategy` (`best`, `latest`, or `all`). When continuing, the orchestrator will inject the correct `--load-model-path` for you.
- Constraints:
  * Keep `vec.num_envs` divisible by `vec.num_workers`.
  * Derive batch sizes from your proposal:
    - `train.batch_size = (env.num_envs × env.num_drones × vec.num_envs) × train.bptt_horizon`
    - `train.minibatch_size = train.batch_size = train.max_minibatch_size`
  * On a 28‑core Mac Studio, prefer `vec.num_workers ≤ 28` and `vec.num_envs ∈ {{28, 56, 84}}`. Switching `train.device` to `cpu` is allowed for diagnostics but will slow runs considerably.
- Training artifacts for the active run live in `$PUFFER_AUTOPILOT_RUN_DIR`; structured metrics write to `$PUFFER_AUTOPILOT_SUMMARY` when available.
- Keep the JSON valid; write `{{}}` if you want the next run to reuse the baseline without overrides.
- Past run folders live under `autopilot/runs/`; skim recent `summary.json` and `notes.txt` entries so proposals consider multi-run trends, not just the latest metrics.
- Before changing anything, pause to form a concrete theory about why the last run behaved as it did, decide how to test that theory, and document the rationale.
- Treat `notes.txt` entries as a running lab log: capture what changed, why you changed it, how it performed, what theory you were testing, and what you plan to try next.
 - If you want to warm-start, set `autopilot.resume_mode` to `continue` and choose `autopilot.resume_from` (`latest` or `best`) so the next run reuses a strong checkpoint instead of starting from scratch.

Execution rules:
- Step 1 is mandatory. Launch {script} immediately (via `bash -lc` from repo root) and allow **at least 15 minutes** of timeout budget so the training can finish.
- Do not replace, skip, or defer this command; wait until it completes and capture stdout/stderr.
- If the command exits early or times out, report the exit code and diagnose before changing configs.

Example override:
```
{{
  "train": {{"learning_rate": 0.0032, "ent_coef": 0.10, "bptt_horizon": 16,
             "batch_size": 28672, "minibatch_size": 28672, "max_minibatch_size": 28672}},
  "env": {{"num_envs": 4, "num_drones": 8}},
  "vec": {{"num_workers": 28, "num_envs": 56}}
}}
```

Experiment Selection Strategy:
- Based on behavioral_analysis.next_focus:
  * "diagnostic_grip" → Set total_timesteps to 100,000-500,000 for quick debugging
  * "improve_grip" → Focus on grip rewards, reduce num_drones for clarity
  * "improve_carrying" → Test stability params, may need longer runs (1M-5M timesteps)
  * "fix_stability" → Reduce learning rate, check collision penalties
  * "optimize_performance" → Full runs (10M+ timesteps) with hyperparameter tuning

Steps:
1. Execute {script} (with ≥15‑minute timeout allowance) and wait for it to finish.
2. Inspect the results (e.g., tail the fresh log under `autopilot/logs/` or read `$PUFFER_AUTOPILOT_SUMMARY` when it exists).
3. CHECK BEHAVIORAL ANALYSIS in the summary to understand what the drone is doing wrong.
4. Based on behavioral insights AND metrics, update `autopilot/proposals/next_config.json` with overrides for the **next** run (or `{{}}` if no change). Match experiment type to the identified problem.
5. Create or overwrite {notes_path} with an insightful status entry. Include:
   - Behavioral analysis insights (what's the drone doing wrong?)
   - Severity and recommended next focus
   - Why you chose specific experiment parameters
   - Theory about fixing the identified issue
