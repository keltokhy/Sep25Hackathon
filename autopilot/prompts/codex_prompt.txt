OVERVIEW & GOALS
- Use behavioral analysis to improve the drone pick‑and‑place task (grip → carry → deliver) by changing environment code, not hyperparameters.
- Honor a strict no‑hyperparameter‑changes policy. Treat `autopilot/journal/notes.md` as long‑term memory and constraints.

THE TASK
Drone must: take off → approach box → hover → descend → grip → carry to drop zone → deliver.
Success requires completing the full chain.

KEY METRICS
- `perfect_grip` / `grip_success`: Successfully gripped box (0-1)
- `perfect_deliv` / `delivery_success`: Successfully delivered (0-1)
- `ho_pickup`: "Hovering for pickup" - drone above box before descent (0-1)
- `de_pickup`: "Descending for pickup" - drone descending toward box (0-1)
- `to_pickup`: "Toward pickup" - moving toward box (0-1)
- `to_drop`: "Toward drop" - carrying box to drop zone (0-1)
- `ho_drop`: "Hovering for drop" - above drop zone (0-1)
- `attempt_grip` / `attempt_drop`: Number of attempts
- `oob`: Out of bounds rate (0-1)
- `collision_rate`: Hit obstacles/floor (0-1)
- `episode_return`: Total reward (higher better)

CONTEXT
- Iteration: {iteration}
- Run ID: {run_id}

OPERATING PRINCIPLES
- First understand WHY before changing WHAT. Trace mathematical relationships in the code.
- Consult `autopilot/journal/notes.md` before acting, especially "Header evolution: `drone_pp.h`" section.
- When metrics worsen after a change, consider reverting harmful additions rather than layering fixes.
- State behavioral hypothesis and expected metrics impact for each change.
- If no improvement after few iterations, try different dimension.
- If performance degrades at consistent training points, suspect curriculum (k parameter) issues - consider making configurable.
- Track which 'dimensions' tried (gates/physics/rewards/spawn/curriculum) and explore untouched ones.
- Before adding new mechanisms, check if existing parameters in .ini can achieve the goal.

SINGLE‑RUN PER ITERATION
- Launch training once per iteration. Edits apply to next iteration's run. No auto-retry on failures.

WORKFLOW (Numbered Core Loop)
1) Execute {script} (autopilot/scripts/run_training.sh) with ≥15‑minute timeout. Do not skip.
2) Read the fresh run's `summary.json` for behavioral_analysis (note: this field is not printed in train.log "User Stats"). Use `trainer_summary.json` or `train.log` for raw metrics.
3) Analyze: Compare metrics across last 3 runs. Are changes helping or hurting? Look for patterns in timing (which epoch fails).
4) Root cause: Identify the specific failure mode from behavioral_analysis and metrics. Ask: is this a symptom or the cause? Trace back through the mathematical formulas.
5) Solution: Make minimal, targeted environment code changes to address the root cause, aligned with the historical intent captured in Notes. If recent changes made things worse, revert them first.
6) Rebuild bindings and smoke‑test import if .h/.c files changed.
7) Configure the next run using only `autopilot.*` overrides (or `{}`) in `autopilot/proposals/next_config.json`.
8) Record actions/observations/outcomes/next steps in labbook and notes; repeat.

DECISION FRAMEWORK
• diagnostic_grip (when perfect_grip = 0):
  - If ho_pickup < 0.1: Drone can't reach hover → relax hover gates or adjust spawn
  - If ho_pickup > 0.1 but de_pickup < 0.1: Hover works but won't descend → check descent gates
  - If de_pickup > 0.1 but perfect_grip = 0: Descending but can't grip → relax grip conditions
  - If metrics improve then degrade at consistent epoch: Curriculum too aggressive → reduce grip_k_decay in drone_pp.ini

• improve_carrying (when perfect_grip > 0 but perfect_deliv = 0):
  - Check to_drop: If low, navigation issue → adjust carry rewards
  - Check ho_drop: If low, can't stabilize at drop → tune drop hover gates

• fix_stability (when oob > 0.5 or collision_rate > 0.1):
  - High OOB often means spawn too close to boundaries or descent too aggressive

CREATIVE ESCALATION (When progress stalls)
- First ask: Have recent changes made metrics WORSE? If yes, revert harmful additions before trying new things.
- Early iterations: Focus on PRIORITY 1 (.ini tweaks) - often the simplest parameter adjustment solves the issue
- Mid iterations: If limited progress, try different dimension in same priority level
- Later iterations: Escalate to PRIORITY 2/3 only if .ini changes insufficient
- After several iterations with minimal improvement, try orthogonal approaches:
  • Parameter tuning: Check if grip_k_decay or other rates need adjustment
  • Reward shaping: Modify reward weights in .ini first, then computation in .h
  • Physics tuning: Adjust box mass, drone thrust, gravity constants
  • Curriculum control: grip_k_decay in .ini controls difficulty progression speed
  • Anti-collapse mechanisms: Add exploration bonuses or success-gated progression
  • Spawn strategy: Alter initial positions/orientations fundamentally

WHAT YOU CAN CHANGE
- Environment code:
  • `drone_pp.h` - Main logic, states, physics, rewards
  • `drone_pp.c` - Core simulation
  • `drone_pp.py` - Observation/action spaces
  • `binding.c` - Python-C interface
  • `dronelib.h` - Physics utilities
  • `drone_pp.ini` - ALL environment parameters including rewards, episode_length, and ANY NEW PARAMETERS YOU ADD (like k_decay_rate, k_start, k_end)
- Autopilot overrides (`autopilot/proposals/next_config.json`):
  • ONLY: resume_mode, resume_from, save_strategy

WHAT YOU CANNOT CHANGE
- Training hyperparameters in proposals (`train.*`, `env.*`, `vec.*`)
- NOTE: drone_pp.ini is NOT a hyperparameter file - it's environment configuration you CAN modify freely

ENVIRONMENT DEBUGGING CHECKLIST
1) Identify failure from behavioral_analysis and concrete metrics.
2) Consult `autopilot/journal/notes.md` → "Header evolution: `drone_pp.h` (Sep 11 → Sep 20, 2025)" to understand prior changes and avoid reversions of removed behaviors (e.g., low‑altitude penalty) unless explicitly justified.
3) Choose the right file to edit (in priority order):
   PRIORITY 1 (Try first - no rebuild needed):
   • `drone_pp.ini` - Modify existing parameters like grip_k_decay (currently 0.09049941256843744)
     This controls how fast difficulty decreases: env->grip_k = tick * -grip_k_decay + grip_k_max
     Lower values = slower decay = more time to learn at each difficulty level
     Math insight: With current values, k drops from 17.9 to 1.0 in ~200k steps. Is that too fast?

   PRIORITY 2 (Most impactful - requires rebuild):
   • `drone_pp.h` - Main logic, state machines, reward computation, physics
     Consider: Environment-side solutions to training dynamics issues

   PRIORITY 3 (When deeper changes needed):
   • `drone_pp.py` - To change observation/action spaces
   • `binding.c` - To expose new metrics for diagnostics

   PRIORITY 4 (Rarely needed):
   • `drone_pp.c` - Core simulation loop
   • `dronelib.h` - Fundamental physics utilities
4) Make a minimal, documented change tied to the hypothesis and historical context.
5) Rebuild: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`.
6) Test: `pytest PufferLib/tests/test_env_binding.py -q` (if present) and ensure import works.
7) Document in `runs/<run_id>/notes.txt`: problem → change → expected outcome; reference the specific historical bullet(s) you followed or intentionally deviated from.

SUCCESS METRICS
- WARNING SIGN: If OOB increases from ~0.85 to >0.95 after a change, that change is harmful - revert it
- Baseline "working" when:
  • grip_success shows majority success
  • delivery_success shows significant rate
  • end_to_end_success shows upward trend
  • OOB/crash rates remain low and stable
- Intermediate milestones (reassess if not met):
  • Early: ho_pickup showing improvement (>0.01 and rising)
  • Mid: de_pickup increasing
  • Later: perfect_grip becoming non-zero
  • Eventually: perfect_grip showing consistent growth
- If milestones stagnate or reverse, first check if recent changes are harmful
- Note: Consistent degradation at same epoch suggests curriculum issues (grip_k_decay parameter)

FAILSAFES
- No behavioral_analysis: Check `summary.json` (not in train.log), add logging if needed
- Build fails: Revert patch, rebuild clean
- Metrics regress multiple runs: Revert to known-good state
- Prefer smaller edits over large refactors

TECHNICAL DETAILS
- Build: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`
- Tests: `pytest PufferLib/tests/test_env_binding.py -q` when available
- Constraints:
  • vec.num_envs % vec.num_workers == 0
  • train.batch_size = env.num_envs × env.num_drones × vec.num_envs × train.bptt_horizon
- Device: Prefer `mps`; use `cpu` only for diagnostics
- Run artifacts: `$PUFFER_AUTOPILOT_RUN_DIR`; logs under `autopilot/logs/`

EXECUTION RULES
- Step 1 is mandatory: Launch {script} immediately from repo root and allow ≥15 minutes. Wait until it completes and capture stdout/stderr. Diagnose early exits before changing configs.
- Run training only once per iteration. Do not start a second training after making edits; those edits are for the next iteration's single run.
- After the run, write the next overrides as `{}` or only `autopilot.*` keys. Do not propose any other hparam changes.

RECORDKEEPING
- Labbook (`autopilot/journal/labbook.md`): actions, observations, outcomes, next steps
- Notes (`autopilot/journal/notes.md`): curated long-term memory, edit in place, keep concise
- Run directories: `autopilot/runs/<run_id>` where `<run_id>` = `YYYY-MM-DDTHHMMSSZ`
- Always reference exact run_id in commits and notes

COMMIT DISCIPLINE (MANDATORY)
- After EVERY single training run, make exactly one commit capturing:
  • Environment code edits
  • Run artifacts under `autopilot/runs/<run_id>/`
  • Journal updates (labbook.md and notes.md)
  • `autopilot/proposals/next_config.json` for next run
- ALWAYS immediately push to current branch. Never skip the push.
- Commit message format: `env(drone_pp): <change> | <run_id>`
  Follow with bullets: problem, change, expected, outcome, next

ENVIRONMENT CHANGE TRACKING
- Each run captures diffs and environment comparison
- Correlate changes with outcomes to decide keep/revert
- Document changes and impacts in run notes

NOTES.MD DISCIPLINE
- Edit in place, keep ≤150 lines, prefer bullets
- Structure:
  1) Current Baseline
  2) Stable Learnings (5-10 bullets)
  3) Header Evolution
  4) Open Questions & Next Hypotheses (≤5 bullets)
  5) Decisions Log (dated bullets with run IDs)
  6) Behavioral Patterns (what worked AND what didn't)
- Division: detailed narratives in labbook.md; notes.md for quick reference
