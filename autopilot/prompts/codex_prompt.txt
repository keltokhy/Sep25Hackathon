OVERVIEW & GOALS
- Use behavioral analysis to improve the drone pick‑and‑place task (grip → carry → deliver) by changing environment code, not hyperparameters.
- Honor a strict no‑hyperparameter‑changes policy. Treat `autopilot/journal/notes.md` as long‑term memory and constraints.

OPERATING PRINCIPLES (Thoughtful, Memory‑Driven)
- Read `autopilot/journal/notes.md` before acting; specifically consult the section “Header evolution: `drone_pp.h` (Sep 11 → Sep 20, 2025)” for historical intent and guardrails. Mirror key learnings to `autopilot/journal/labbook.md` after each run.
- For non‑trivial work, write a brief 2–4 step plan, then execute and verify against constraints (divisibility, batch math).
- Always state the behavioral hypothesis you are testing and the expected effect on grip/hover/carry/deliver metrics.
- If guidance conflicts, prefer the latest committed environment code and the Notes file.
- Be introspective: do not make assumptions—check yourself. Verify claims against the current code, config, and logs before acting, and re‑read constraints after changes.

SINGLE‑RUN PER ITERATION (Strict)
- Launch training exactly once at the start of the iteration, then do analysis and documentation only. Do not relaunch training again within the same iteration.
- Any environment edits you make apply to the next iteration’s single training run.
- If the run fails or times out, record the failure and end the iteration; do not auto‑retry.

WORKFLOW (Numbered Core Loop)
1) Execute {script} (autopilot/scripts/run_training.sh) with ≥15‑minute timeout. Do not skip.
2) Read the fresh run’s `trainer_summary.json` or mirrored `train.log` and extract behavioral_analysis and key metrics.
3) Identify the specific failure mode from behavioral_analysis and metrics. Cross‑check against the “Header evolution: `drone_pp.h`” notes to avoid re‑introducing patterns previously removed.
4) Make minimal, targeted environment code changes to address the failure, aligned with the historical intent captured in Notes.
5) Rebuild bindings and smoke‑test import.
6) Configure the next run using only `autopilot.*` overrides (or `{}`) in `autopilot/proposals/next_config.json`.
7) Record actions/observations/outcomes/next steps in labbook and notes; repeat.

DECISION FRAMEWORK
- Read behavioral_analysis first. Use it to choose an environment‑focused response:
  • diagnostic_grip: Agents aren’t attempting/achieving grip → relax grip gates slightly, refine target hover point, add attempt counters/logs.
  • improve_carrying: Grip works but drops/fails delivery → tune carry stability (mass, drag, grip decay), adjust delivery success gating.
  • fix_stability/high_collisions: Crashes/OOB high → adjust physics constants, collision penalties, spawn spacing, jitter handling.
  • optimize_performance (only after success thresholds met): Improve throughput/observability, not hparams; add logging, keep environment stable.

WHAT YOU CAN CHANGE
- Environment code (primary): e.g., `PufferLib/pufferlib/ocean/drone_pp/drone_pp.h` and adjacent files when necessary.
- Autopilot settings only via overrides file: `autopilot/proposals/next_config.json` keys permitted: 
  • `autopilot.resume_mode` = `fresh` | `continue`
  • `autopilot.resume_from` = `latest` | `best` | explicit checkpoint path
  • `autopilot.save_strategy` = `best` | `latest` | `all`
- Notes and labbook entries.

WHAT YOU CANNOT CHANGE (Hard Rule)
- Do not modify any `train.*`, `env.*`, or `vec.*` hyperparameters in proposals. Under current policy these are off‑limits; the orchestrator ignores them.
- The training script may enforce derived constraints (batch sizing, vec divisibility) for correctness; this is not a policy exception and should not be edited.

ENVIRONMENT DEBUGGING CHECKLIST
1) Identify failure from behavioral_analysis and concrete metrics.
2) Consult `autopilot/journal/notes.md` → “Header evolution: `drone_pp.h` (Sep 11 → Sep 20, 2025)” to understand prior changes and avoid reversions of removed behaviors (e.g., low‑altitude penalty) unless explicitly justified.
3) Locate relevant code in `drone_pp.h` (hover targeting, grip gates, delivery checks, physics constants).
4) Make a minimal, documented change tied to the hypothesis and historical context.
5) Rebuild: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force`.
6) Test: `pytest PufferLib/tests/test_env_binding.py -q` (if present) and ensure import works.
7) Document in `runs/<run_id>/notes.txt`: problem → change → expected outcome; reference the specific historical bullet(s) you followed or intentionally deviated from.

SUCCESS METRICS (Define thresholds)
- Baseline environment “working” when, over a stable eval slice:
  • grip_success ≥ 60% and delivery_success ≥ 40% across agents/episodes.
  • end_to_end_success shows an upward trend across 2–3 consecutive runs.
  • OOB/crash resets ≤ 10% of steps; collision‑induced terminations ≤ 5% episodes.
- Stop environment debugging when thresholds are met and metrics are stable; shift to performance/observability improvements only (no hparam tuning).

FAILSAFES
- No behavioral_analysis in summary: fall back to `train.log` inspection; compute basic rates (grips, deliveries, OOB). If absent, add logging counters in env and rerun.
- Build fails: revert the last env patch, rebuild clean, and re‑apply a smaller, testable change.
- Metrics regress for ≥2 runs after a change: revert to previous known‑good env state (use git restore to last good commit) and document rollback.
- When in doubt, prefer smaller env edits and clearer logging over larger refactors.

TECHNICAL DETAILS
- File locations: environment code under `PufferLib/pufferlib/ocean/…` (primary: `drone_pp.h`), configs under `autopilot/configs/`, prompts under `autopilot/prompts/`.
- Build: `cd PufferLib && NO_TRAIN=1 python3 setup.py build_ext --inplace --force` (rebuild native bindings after `.h` changes).
- Tests: `pytest PufferLib/tests/test_env_binding.py -q` when available; otherwise smoke‑import.
- Constraints the script enforces before training:
  • `vec.num_envs % vec.num_workers == 0`.
  • `train.batch_size = env.num_envs × env.num_drones × vec.num_envs × train.bptt_horizon`.
  • `train.minibatch_size = train.batch_size = train.max_minibatch_size`.
- Device: Prefer `mps`; switch to `cpu` only for diagnostics and note slower runtime.
- Run artifacts: `$PUFFER_AUTOPILOT_RUN_DIR`; logs under `autopilot/logs/`.

EXECUTION RULES
- Step 1 is mandatory: Launch {script} immediately from repo root and allow ≥15 minutes. Wait until it completes and capture stdout/stderr. Diagnose early exits before changing configs.
- Run training only once per iteration. Do not start a second training after making edits; those edits are for the next iteration’s single run.
- After the run, write the next overrides as `{}` or only `autopilot.*` keys. Do not propose any other hparam changes.

RECORDKEEPING
- Labbook (`autopilot/journal/labbook.md`): actions, observations (SPS/CPU%), outcome, next step.
- Notes (`autopilot/journal/notes.md`): curated long‑term memory. Edit in place; keep concise and up to date.
- Drive the training toward better drone behavior (grip → carry → deliver) by understanding what's failing and targeting specific issues.
- Use behavioral analysis to make intelligent decisions about experiment type and focus.
- Honor a strict no‑hyperparameter‑changes policy; use the provided baselines as‑is.

RUN NAMING STANDARD
- Run directories live under `autopilot/runs/<run_id>` where `<run_id>` is a UTC timestamp in ISO‑8601 Zulu without colons: `YYYY-MM-DDTHHMMSSZ` (e.g., `2025-09-21T012845Z`).
- Do not rename run folders. Always reference the exact `<run_id>` in commit messages, labbook entries, and notes.
- Logs follow the training script’s naming and are not committed; all other artifacts in the run folder are committed each iteration.

COMMIT DISCIPLINE (Atomic Per Iteration)
- After the single training run and your analysis, make exactly one commit capturing:
  • Any environment code edits (e.g., `PufferLib/pufferlib/ocean/drone_pp/drone_pp.h` and adjacent files).
  • Run artifacts under `autopilot/runs/<run_id>/` (config.json, summary.json, trainer_summary.json, train.log, notes.txt).
  • Journal updates: `autopilot/journal/labbook.md` and `autopilot/journal/notes.md` (curated).
  • `autopilot/proposals/next_config.json` for the next run (usually `{}` or autopilot.* only).
- Immediately push the commit to the current branch (e.g., `git push -u origin HEAD`) so the remote stays in sync each iteration.
- Commit message format:
  `env(drone_pp): <brief change>; expected effect | <run_id>`
  Follow with short bullets:
  - Problem observed: <key metrics>
  - Change: <what/where in .h>
  - Expected impact: <hypothesis>
  - Outcome: <core metrics from trainer_summary>
  - Next: <one action or `{}`>
- Do not start another training inside the same iteration after committing. Edits are for the next iteration’s single run.

ENVIRONMENT CHANGE TRACKING (DREX Feature)
- Each run captures diffs of environment code (e.g., `env_uncommitted.diff`).
- Summaries may include `environment_comparison` vs the previous run.
- Use these to correlate env changes with outcomes:
  • Grip improves after grip‑gate tweak → keep it.
  • Collision rate spikes after physics change → revert/adjust.
  • Track which files were modified: `drone_pp.h`, `drone_pp.c`, bindings.
Document what changed, why, expected vs actual impact in `runs/<run_id>/notes.txt`.

NOTES.MD DISCIPLINE (Curated, Concise)
- Purpose: `autopilot/journal/notes.md` is the single source of stable intent and learnings. It is not a dump log.
- Edit‑in‑place rules:
  • Update or replace outdated bullets; prune stale hypotheses.
  • Keep it short (aim ≤ 150 lines). Prefer crisp bullets over paragraphs.
  • Reference concrete run IDs when relevant (e.g., 2025‑09‑20T233055Z).
- Recommended structure:
  1) Current Baseline: env header commit, config profile, device, vec/env concurrency.
  2) Stable Learnings: 5–10 bullets of proven heuristics that should persist.
  3) Header Evolution: `drone_pp.h` summary (keep high‑level; update only when upstream changes).
  4) Open Questions & Next Hypotheses: ≤5 bullets you intend to test next.
  5) Decisions Log: terse dated bullets linking to runs (decision → rationale → outcome).
- Division of labor:
  • Put detailed, per‑run narratives in `labbook.md` and `runs/<run_id>/notes.txt`.
  • Keep `notes.md` focused on what must guide future iterations at a glance.
